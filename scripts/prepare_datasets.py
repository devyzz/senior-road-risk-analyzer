import sys
import os
import numpy as np
import pandas as pd
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
from pathlib import Path
from sklearn.neighbors import BallTree
from sklearn.cluster import DBSCAN
import utils.GEO_UTILS as GU
import utils.CONSTANTS as CONST


def process_traffic_volume_combined(accident_df: pd.DataFrame, year: int) -> pd.DataFrame:
    traffic_df = pd.read_csv("./data/raw/traffic_mereg_data.csv")

    print(f"=> {year}ë…„ ì‚¬ê³  + êµí†µëŸ‰ ë³‘í•© ì¤‘...")

    traffic_coords = traffic_df[["lat", "lng", "2021", "2022", "2023"]].copy()
    acc_coords = accident_df[["lat", "lng"]].values
    acc_years = accident_df["acdnt_year"].astype(str).values

    tree = BallTree(np.radians(traffic_coords[["lat", "lng"]].values), metric='haversine')
    distances, indices = tree.query(np.radians(acc_coords), k=1)

    distances_km = distances.flatten() * 6371
    indices = indices.flatten()

    traffic_values = []
    for i, dist in enumerate(distances_km):
        year_str = acc_years[i]
        if dist > 2.0:
            traffic_values.append(np.nan)
            continue

        nearest = traffic_coords.iloc[indices[i]]
        val = nearest[year_str]
        if pd.isna(val):
            fallback = [y for y in ["2021", "2022", "2023"] if y != year_str]
            val = pd.to_numeric(nearest[fallback], errors='coerce').mean()
        traffic_values.append(val)

    accident_df["traffic_volume"] = traffic_values
    return accident_df

def run_all_processing_steps(year):
    print(f"==> {year}ë…„ ë°ì´í„° í†µí•© ì²˜ë¦¬ ì‹œì‘...\n")

    accident_path = f"./data/raw/all_accident_info_{year}.csv"
    accident_df = pd.read_csv(accident_path)

    # 1. íš¡ë‹¨ë³´ë„ ì¡´ì¬ìœ ë¬´ ì»¬ëŸ¼ë³‘í•©
    crosswalk_df = pd.read_csv("./data/external/crosswalk_data.csv")
    accident_df = GU.mark_zone_proximity_common(
        accident_df, crosswalk_df,
        zone_lat_col="ìœ„ë„", zone_lng_col="ê²½ë„",
        output_col="near_crosswalk",
        radius_m=15, buffer_m=10
    )

    # 2. ì‹ í˜¸ë“± ì¡´ì¬ìœ ë¬´ ì»¬ëŸ¼ë³‘í•©
    light_df = pd.read_csv("./data/external/traffic_light_data.csv")
    accident_df = GU.mark_zone_proximity_common(
        accident_df, light_df,
        zone_lat_col="ìœ„ë„", zone_lng_col="ê²½ë„",
        output_col="near_traffic_light",
        radius_m=50, buffer_m=25
    )

    # 2. ë³´í˜¸êµ¬ì—­ ì¡´ì¬ìœ ë¬´ ì»¬ëŸ¼ ë³‘í•©
    zone_df = pd.read_csv("./data/external/protection_zone_data.csv")
    for zone_type, col_name in CONST.ZONE_COLUMNS.items():
        sub_df = zone_df[zone_df["êµ¬ë¶„"] == zone_type]
        accident_df = GU.mark_zone_proximity_common(
            accident_df, sub_df,
            zone_lat_col="ìœ„ë„", zone_lng_col="ê²½ë„",
            output_col=col_name,
            radius_m=300, buffer_m=100
        )

    # 3. ë„ë¡œëª…, ì°¨ë¡œìˆ˜ ì»¬ëŸ¼ ì •ì œ / ì†ë„ ì •ë³´ ì¶”ê°€
    def refine(df):
        df = df[df["ë„ë¡œëª…"].notna() & (df["ë„ë¡œëª…"].str.strip() != "")]
        df["ë„ë¡œëª…"] = df["ë„ë¡œëª…"].str.replace(" ", "").str.strip()
        
        def parse_lanes(val):
            try:
                parts = str(val).strip().split("~")
                nums = list(map(int, parts))
                return round(sum(nums) / len(nums))
            except:
                return np.nan

        df["ì°¨ë¡œìˆ˜"] = df["ì°¨ë¡œìˆ˜"].apply(parse_lanes)
        return df

    def pick_velocity(row):
        code = row["occrrnc_time_code"]
        return (
            row["ì˜¤ì „"] if 6 < code < 10 else
            row["ë‚®"] if 11 < code < 14 else
            row["ì˜¤í›„"] if 16 < code < 20 else
            row["ì „ì¼"]
        )

    velocity_df = refine(pd.read_csv(f"./data/raw/{year}velocity.csv"))
    merged_velocity = pd.merge(accident_df, velocity_df, how="inner", left_on="route_nm", right_on="ë„ë¡œëª…")
    merged_velocity["lanes"] = merged_velocity["ì°¨ë¡œìˆ˜"]
    merged_velocity["lengths"] = pd.to_numeric(merged_velocity["ì—°ì¥"].str.replace(",", ""), errors="coerce")
    merged_velocity["velocity"] = merged_velocity.apply(pick_velocity, axis=1)
    accident_df = pd.merge(
        accident_df,
        merged_velocity[["acdnt_no", "lanes", "lengths", "velocity"]],
        how="left",
        on="acdnt_no"
    )
    
    # 4. êµí†µëŸ‰
    accident_df = process_traffic_volume_combined(accident_df, year)

    # ì €ì¥
    output_path = f"./data/processed/accident_data_{year}.csv"
    accident_df.to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"====> ìµœì¢… ì €ì¥ ì™„ë£Œ: {output_path}\n")
    
def merge_all_years():
    print("===> ì—°ë„ë³„ CSV ì „ì²´ ë³‘í•© ì¤‘...")
    base_dir = "./data/processed"
    file_names = [f"accident_data_{y}.csv" for y in range(2021, 2024)]
    
    # âš ï¸ DtypeWarning ë°©ì§€: low_memory=False ì˜µì…˜ ì¶”ê°€
    dfs = [pd.read_csv(os.path.join(base_dir, f), low_memory=False) for f in file_names]
    merged_df = pd.concat(dfs, ignore_index=True)
    
    before_rows = len(merged_df)
    remove_targets = ["ìì „ê±°", "ê°œì¸í˜•ì´ë™ìˆ˜ë‹¨(PM)"]
    merged_df = merged_df[~merged_df["wrngdo_vhcle_asort_dc"].isin(remove_targets)]
    after_rows = len(merged_df)
    print(f"ğŸš² 'ìì „ê±°' ìš´ì „ì í–‰ ì œê±°ë¨: {before_rows - after_rows}ê±´")
    
    merged_df.to_csv(os.path.join(base_dir, "accident_data_all.csv"), index=False, encoding="utf-8-sig")
    print("====> ë³‘í•© ì™„ë£Œ: accident_data_all.csv")    
    
    return merged_df
    
def filter_all_data():
    base_dir = "./data/processed"
    merged_file = os.path.join(base_dir, "accident_data_all.csv")

    if not Path(merged_file).exists():
        print(f"âš ï¸ ë³‘í•© íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {merged_file}")
        return

    df = pd.read_csv(merged_file, low_memory=False)

    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    columns_to_keep = [
        "acdnt_year", "occrrnc_time_code", "legaldong_name", "acdnt_hdc",
        "lrg_violt_1_dc", "road_stle_dc", "wrngdo_vhcle_asort_dc", "acdnt_age_1_code", #"acdnt_age_1_dc",
        "rdse_sttus_dc", "road_div", "lat", "lng",
        "near_crosswalk", "near_traffic_light", "near_child_zone",
        "near_elderly_zone", "near_disabled_zone",
        "lanes", "lengths", "velocity", "traffic_volume",
        "elderly_hotspot","non_elderly_hotspot","all_hotspot","route_nm"
    ]
    filtered_columns = [col for col in columns_to_keep if col in df.columns]
    filtered_df = df[filtered_columns]

    # ì €ì¥
    output_path = os.path.join(base_dir, "accident_data_filtered.csv")
    filtered_df.to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"===> í•„í„°ë§ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}")    

if __name__ == "__main__":
    #1. ì—°ë„ë³„ ì‚¬ê³  ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘
    for year in [2021, 2022, 2023]:
        run_all_processing_steps(year)
    
    #2. ë°ì´í„°í†µí•© ë° ì‚¬ê³ ë‹¤ë°œ êµ¬ì—­ ì»¬ëŸ¼ ì¶”ê°€
    all_df = merge_all_years()
    GU.assign_hotspot_columns(all_df, lat_col='lat', lon_col='lng', id_col='acdnt_no')
    
    #3. ë°ì´í„° í•„í„°ë§
    filter_all_data()
